_target_: src.data.datamodule.DataModule

num_classes: 1000

train_dataset:
  _target_: src.data.imagenet.ImageNet
  _partial_: True

  root: ${core.data_dir}/imagenet
  split: train

  transform:
    _target_: torchvision.transforms.v2.Compose
    transforms:
      - _target_: torchvision.transforms.v2.ToImage
      - _target_: torchvision.transforms.v2.ToDtype
        dtype:
          _target_: torch.__dict__.get # workaround, see: https://github.com/facebookresearch/hydra/issues/2140
          _args_:
            - float32
        scale: True
      - _target_: torchvision.transforms.v2.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
      - _target_: torchvision.transforms.v2.RandomCrop
        size: 224
        pad_if_needed: True

val_dataset:
  _target_: src.data.imagenet.ImageNet
  _partial_: True

  root: ${core.data_dir}/imagenet
  split: val

  transform:
    _target_: torchvision.transforms.v2.Compose
    transforms:
      - _target_: torchvision.transforms.v2.ToImage
      - _target_: torchvision.transforms.v2.ToDtype
        dtype:
          _target_: torch.__dict__.get # workaround, see: https://github.com/facebookresearch/hydra/issues/2140
          _args_:
            - float32
        scale: True
      - _target_: torchvision.transforms.v2.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
      - _target_: torchvision.transforms.v2.CenterCrop
        size: 224

batch_size: 32
num_workers: 8
pin_memory: True
